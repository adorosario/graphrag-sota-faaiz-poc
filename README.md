# GraphRAG Document Processing System

An intelligent, lightweight document processing system powered by OpenAI GPT-5 and advanced retrieval techniques for unprecedented insights into your data.

## Table of Contents
- [System Architecture](#system-architecture)
- [Core Modules](#core-modules)
- [Data Processing Flow](#data-processing-flow)
- [Installation & Setup](#installation--setup)
- [Quick Start](#quick-start)
- [Troubleshooting](#troubleshooting)

## System Architecture

### Overview
GraphRAG is a next-generation Retrieval Augmented Generation (RAG) system that intelligently routes queries to the optimal retrieval method: Vector Search, Graph Traversal, or Hybrid approach. This lightweight implementation uses OpenAI APIs instead of heavy local ML models, resulting in **10x faster builds**, **90% smaller container size**, and **superior accuracy**.

### Key Features
- **üöÄ Ultra-lightweight**: 50MB container vs 1GB+ traditional implementations
- **‚ö° Lightning fast**: 3-minute builds vs 20+ minute ML model downloads
- **üß† GPT-5 powered**: State-of-the-art entity extraction and embeddings
- **üéØ Intelligent Query Routing**: Automatically selects the best retrieval strategy
- **üìö Multi-format Support**: Processes PDF, CSV, TXT, DOCX, and MD files
- **üîÑ Incremental Processing**: Only processes new or modified documents
- **üìä Knowledge Graph Construction**: Builds Neo4j graph with entities and relationships
- **üìà Performance Analytics**: Real-time monitoring and reporting
- **üèóÔ∏è Production Ready**: Docker-based deployment with SSL support

## Core Modules

### Module Architecture and Data Flow

#### 1. Data Ingestion Pipeline (`data_ingestion_pipeline.py`)

**Purpose**: Processes documents and builds the knowledge graph using OpenAI APIs

**Key Components**:

- **DataIngestionPipeline Class**
  - File hash tracking using MD5 for incremental processing
  - Multi-format document reader supporting PDF, CSV, TXT, DOCX, MD
  - Automatic document type detection based on file extension
  - Lazy processing - only processes changed files

- **OpenAI-Powered Entity Extraction**
  - Uses **GPT-5** for Named Entity Recognition (configurable)
  - Extracts: People (PERSON), Organizations (ORG), Locations (GPE), Events, Products
  - Superior accuracy compared to spaCy local models
  - Generates unique IDs using MD5 hashing
  - Limits to MAX_ENTITIES_PER_CHUNK (default: 10) for performance

- **Relationship Extraction**
  - Pattern-based extraction using regex
  - Relationship types: IS_A, WORKS_FOR, FOUNDED, LOCATED_IN, OWNS
  - Confidence scoring (0.9 for GPT-based matches)
  - Context preservation for each relationship

- **Neo4jGraphManager Class**
  - Creates optimized schema with constraints and indexes
  - Batch processing for efficient insertion
  - Full-text search index creation
  - Graph statistics monitoring

**Processing Flow**:
```
1. Directory Scan ‚Üí 2. Hash Check ‚Üí 3. Document Loading ‚Üí 4. Content Extraction
‚Üí 5. GPT-5 Entity Recognition ‚Üí 6. Relationship Detection ‚Üí 7. Graph Node Creation
‚Üí 8. OpenAI Embeddings ‚Üí 9. Index Creation ‚Üí 10. Cache Update
```

#### 2. Query Router System (`query_router_system.py`)

**Purpose**: Intelligently routes queries to optimal retrieval method using OpenAI embeddings

**Routing Logic**:

- **QueryFeatureExtractor Class**
  - Extracts 8 key features from queries:
    1. Entity count (capitalized words)
    2. Token count
    3. Multi-hop indicators (relationship patterns)
    4. Relationship words presence
    5. Complex reasoning patterns
    6. Question type (what/how/why/who/where/when)
    7. Complexity score (0-1 scale)
    8. Named entities list

- **AgenticQueryRouter Class**
  - **Vector Route** (complexity < 0.3):
    - Simple factual queries
    - No relationship indicators
    - Example: "What is IBM?"
    - Uses ChromaDB with OpenAI embeddings
  
  - **Graph Route** (entities ‚â• 2, complexity ‚â• 0.4):
    - Relationship-focused queries
    - Contains relationship words
    - Example: "How are X and Y connected?"
    - Uses Neo4j graph traversal
  
  - **Hybrid Route** (complexity ‚â• 0.6):
    - Complex multi-hop queries
    - Multiple entities with relationships
    - Example: "How did A affect B over time?"
    - Combines both methods with RRF

**Retrieval Components**:

- **VectorRetriever Class**
  - ChromaDB persistent storage at `./chroma_db`
  - **OpenAI text-embedding-3-large** (configurable)
  - Cosine similarity scoring
  - Automatic Neo4j synchronization on init

- **GraphRetriever Class**
  - Entity-based traversal with co-occurrence detection
  - Full-text search fallback
  - Simple content search as final fallback
  - Cypher query optimization

- **HybridRetriever Class**
  - Reciprocal Rank Fusion (RRF) with k=60
  - Content-based deduplication
  - Source tracking and fusion metadata
  - Performance metrics collection

#### 3. Runtime Processor (`runtime_processor.py`)

**Purpose**: Command-line tool for batch document processing

**Features**:
- Directory validation and analysis
- File type detection and counting
- Progress tracking with detailed logging
- Automatic report generation in JSON
- Cache management for incremental processing

**Processing Metrics Tracked**:
- Documents processed
- Entities created (via GPT-5)
- Relationships created
- Processing time
- Files skipped (unchanged)
- Graph statistics

#### 4. Demo Application (`demo_app.py`)

**Purpose**: Interactive web interface with Streamlit

**Key Features**:

- **Authentication System**
  - Environment-based credentials
  - Multi-user support
  - Session management

- **Document Upload**
  - Drag-and-drop interface
  - 50MB file size limit
  - Automatic processing pipeline
  - ChromaDB synchronization

- **Query Interface**
  - Real-time routing visualization
  - Confidence scoring display
  - Processing time metrics
  - Top-5 results with metadata

- **Performance Analytics**
  - Route distribution pie chart
  - Processing time trend graph
  - Session statistics
  - Query history table

### Data Processing Flow

#### Document Ingestion Flow
```
1. User uploads document via Streamlit UI
2. File saved to /app/data/uploads/
3. runtime_processor.py called with upload directory
4. Hash check performed (MD5)
5. If new/modified:
   - Content extracted using LlamaIndex
   - Entities extracted using GPT-5 API
   - Embeddings generated using OpenAI API
   - Relationships detected via patterns
   - Neo4j nodes created (Document, Entity)
   - CONTAINS relationships established
   - ChromaDB synchronized with OpenAI embeddings
6. Processing report generated
7. UI updated with new document count
```

#### Query Processing Flow
```
1. User enters query
2. Feature extraction (8 features)
3. Complexity scoring
4. Route decision (Vector/Graph/Hybrid)
5. Retrieval execution:
   - Vector: ChromaDB similarity search (OpenAI embeddings)
   - Graph: Neo4j traversal + co-occurrence
   - Hybrid: Both + RRF fusion
6. Results ranked and returned
7. Metrics tracked for KPIs
```

## Installation & Setup

### Prerequisites
- Ubuntu 20.04+ or similar Linux distribution
- Docker 20.10+ and Docker Compose 1.29+
- **2GB+ RAM** (reduced from 4GB due to lightweight architecture)
- 10GB+ free disk space (reduced from 20GB)
- Domain name with SSL certificate (for production)
- Neo4j Aura account (free tier works)
- **OpenAI API account with GPT-5 access**

### Environment Setup

1. **Clone the Repository**
```bash
git clone https://github.com/yourusername/graphrag-poc.git
cd graphrag-poc
```

2. **Create Environment File**
```bash
cp .env.sample .env
```

3. **Configure Environment Variables**
Edit `.env` with your settings:
```env
# Neo4j Configuration (REQUIRED)
NEO4J_URI=neo4j+s://your-instance.databases.neo4j.io
NEO4J_USERNAME=neo4j
NEO4J_PASSWORD=your-secure-password

# OpenAI API Configuration (REQUIRED)
OPENAI_API_KEY=sk-your-openai-api-key-here
OPENAI_EMBEDDING_MODEL=text-embedding-3-large
OPENAI_CHAT_MODEL=gpt-5

# Demo Credentials
DEMO_USERNAME_1=admin
DEMO_PASSWORD_1=your-admin-password
DEMO_USERNAME_2=demo
DEMO_PASSWORD_2=your-demo-password

# Application Settings
STREAMLIT_PORT=8501
```

### Docker Deployment

1. **Build and Start Services** (Now lightning fast!)
```bash
# Build Docker images (completes in ~3 minutes)
docker-compose build

# Start services
docker-compose up -d

# Check status
docker-compose ps
```

2. **Initialize Sample Data**
```bash
# Create sample documents
docker exec graphrag-app python -c "
from pathlib import Path
sample_dir = Path('/app/data/sample')
sample_dir.mkdir(parents=True, exist_ok=True)

with open(sample_dir / 'test.txt', 'w') as f:
    f.write('TechCorp is a technology company founded by John Smith.')
"

# Process sample data (uses GPT-5 for entity extraction)
docker exec graphrag-app python runtime_processor.py /app/data/sample
```

3. **Apply ChromaDB Fix**
```bash
# Copy fix script to container
docker cp fix_chromadb_sync.py graphrag-app:/app/

# Run the fix
docker exec graphrag-app python /app/fix_chromadb_sync.py

# Restart services
docker-compose restart
```

### Verification

1. **Check Database Sync**
```bash
# Neo4j document count
docker exec graphrag-neo4j cypher-shell -u neo4j -p 'your-password' \
  "MATCH (d:Document) RETURN count(d) as count;"

# ChromaDB document count
docker exec graphrag-app python -c "
import chromadb
client = chromadb.PersistentClient(path='/app/chroma_db')
collection = client.get_collection('documents')
print(f'ChromaDB: {collection.count()} documents')
"
```

## Quick Start

### 1. Access the Application
Navigate to `https://your-domain.com` (or `http://localhost:8501` for local)

### 2. Login
Use credentials from your `.env` file

### 3. Upload Documents
1. Click "üìÅ Document Upload" in sidebar
2. Select files (PDF, CSV, TXT, DOCX, MD)
3. Click "Process Uploaded Documents"
4. Wait for GPT-5 processing completion

### 4. Test Queries

**Simple Queries (Vector Route)**:
```
What is TechCorp?
What was the revenue in 2023?
Who is the Knowledge Graph?
```

**Relationship Queries (Graph Route)**:
```
How is John Smith related to TechCorp?
What is the connection between Product A and Department B?
Show relationships for Mary Johnson
```

**Complex Queries (Hybrid Route)**:
```
How did the technology changes affect financial performance over the last 3 years?
Compare our product strategy with market trends and analyze the impact
What factors led to the revenue growth and how did they influence each department?
```

### 5. Monitor Performance
- View route distribution in sidebar
- Check processing times in charts
- Review query history table

## Troubleshooting

### OpenAI API Issues

**Symptoms**: Entity extraction or embedding failures

**Solution**:
```bash
# Check API key
docker exec graphrag-app python -c "
import os
import openai
openai.api_key = os.getenv('OPENAI_API_KEY')
try:
    response = openai.models.list()
    print('‚úÖ OpenAI API connected')
    print(f'Available models: {[m.id for m in response.data[:3]]}')
except Exception as e:
    print(f'‚ùå OpenAI API error: {e}')
"

# Verify GPT-5 access
docker exec graphrag-app python -c "
import os
import openai
openai.api_key = os.getenv('OPENAI_API_KEY')
try:
    response = openai.chat.completions.create(
        model='gpt-5',
        messages=[{'role': 'user', 'content': 'Hello'}],
        max_tokens=5
    )
    print('‚úÖ GPT-5 access confirmed')
except Exception as e:
    print(f'‚ùå GPT-5 access denied: {e}')
    print('Consider using gpt-4o as fallback')
"
```

### ChromaDB Not Syncing with Neo4j

**Symptoms**: New documents appear in Neo4j but not in search results

**Solution**:
```bash
# Run the complete fix
docker exec graphrag-app python /app/fix_chromadb_sync.py

# Restart application
docker-compose restart

# Verify sync
docker exec graphrag-app python -c "
from neo4j import GraphDatabase
import chromadb
import os

driver = GraphDatabase.driver(
    os.getenv('NEO4J_URI'),
    auth=(os.getenv('NEO4J_USERNAME'), os.getenv('NEO4J_PASSWORD'))
)

with driver.session() as session:
    neo4j_count = session.run('MATCH (d:Document) RETURN count(d) as count').single()['count']

client = chromadb.PersistentClient(path='/app/chroma_db')
collection = client.get_collection('documents')
chroma_count = collection.count()

print(f'Neo4j: {neo4j_count}, ChromaDB: {chroma_count}')
print('‚úÖ Synced' if neo4j_count == chroma_count else '‚ùå Mismatch')
"
```

### Document Processing Fails

**Check logs**:
```bash
docker-compose logs -f app
docker exec graphrag-app ls -la /app/logs/
```

**Force reprocess**:
```bash
docker exec graphrag-app rm -f /app/cache/file_hashes.json
docker exec graphrag-app python runtime_processor.py /app/data/uploads --force-reprocess
```

### Memory Issues (Now Much Reduced)

**Adjust Docker limits** (now much lower requirements):
```yaml
# docker-compose.yml
services:
  app:
    mem_limit: 2g  # Reduced from 4g
    memswap_limit: 2g
```

## Performance Optimization

### Tuning Parameters

Edit `.env` for optimization:
```env
# OpenAI Models (configurable)
OPENAI_EMBEDDING_MODEL=text-embedding-3-large  # or text-embedding-3-small for cost savings
OPENAI_CHAT_MODEL=gpt-5                        # or gpt-4o for fallback

# Processing
CHUNK_SIZE=512          # Smaller for detailed extraction
CHUNK_OVERLAP=50        # Increase for better context
MAX_ENTITIES_PER_CHUNK=10  # Limit entity extraction

# Router Thresholds
VECTOR_MAX_COMPLEXITY=0.3
GRAPH_MIN_ENTITIES=2
GRAPH_MIN_COMPLEXITY=0.4
HYBRID_MIN_COMPLEXITY=0.6

# Performance
BATCH_SIZE=32           # Adjust based on memory
MAX_WORKERS=4           # Based on CPU cores
```

### Scaling Recommendations

- **Horizontal Scaling**: Deploy multiple app instances behind load balancer
- **Database Optimization**: Use Neo4j Enterprise for clustering
- **Caching**: Implement Redis for query result caching
- **CDN**: Use CloudFlare for static assets
- **OpenAI Rate Limits**: Monitor API usage and implement rate limiting

## Architecture Benefits

### Before (Heavy ML Models)
- ‚ùå **1GB+ container size**
- ‚ùå **20+ minute builds**
- ‚ùå **4GB+ RAM requirements**
- ‚ùå **GPU recommended**
- ‚ùå **Complex model management**
- ‚ùå **Local accuracy limitations**

### After (OpenAI APIs)
- ‚úÖ **~50MB container size**
- ‚úÖ **3-minute builds**
- ‚úÖ **2GB RAM sufficient**
- ‚úÖ **CPU-only deployment**
- ‚úÖ **Zero model management**
- ‚úÖ **State-of-the-art accuracy**

## Support

For issues or questions, please open an issue on GitHub or contact the development team.

---

**Version**: 2.0.0 (OpenAI GPT-5 Edition)  
**Last Updated**: August 2025  
**Status**: Production Ready - Ultra-Lightweight